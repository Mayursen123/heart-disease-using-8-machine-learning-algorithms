# -*- coding: utf-8 -*-
"""heart-disease-using-8-machine-learning-algorithms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTeNc0uH7fpJVa0PKD4HiUkQ7A8uAsdw

heart disease using 8 machine learning algorithms :
1. Linear Regression
2. Logistic Regression
3. support vactor machine
4. naive-bayes
5. XGBoost
6. Random Forest Regression
7. Gradient Boosting Regresso
8. decision tree
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

heart =pd.read_csv('/content/dataset.csv')
heart.head()

"""# data info"""

heart.info()

heart.describe()

heart.columns

"""About this file
age = age in years / min = 29 - max = 77
sex = (1 = male; 0 = female)
cp= chest pain type
trestbps= resting blood pressure (in mm Hg on admission to the hospital)
chol = serum cholestoral in mg/dl
fbs = ( fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
restecg = resting electrocardiographic results
thalach = maximum heart rate achieved
exang = exercise induced angina (1 = yes; 0 = no)
oldpeak = ST depression induced by exercise relative to rest
slope = the slope of the peak exercise ST segment
ca = number of major vessels (0-3) colored by flourosopy
thal3 = normal; 6 = fixed defect; 7 = reversable defect
target = 1 or 0

# finding missing value
"""

heart.isnull().sum()

"""there is no missing value so our data is perfect and clean

# map coorelation
"""

heart.corr()

heart=heart.drop('fbs',axis=1)

"""# vizualisation"""

matplotlib.rcParams['figure.figsize'] =[10,7]
matplotlib.rcParams.update({'font.size': 12})
matplotlib.rcParams['font.family'] = 'sans-serif'

heart.hist(figsize=(15,15),edgecolor='black');

"""# A) target"""

sns.countplot(x='target',data=heart)
plt.title('target = 1 or 0');

"""# A) gendre (male female)"""

sns.countplot(x='sex',hue='target',data=heart)
plt.title('1 = positif; 0 = negatif')
plt.xlabel('sex => 1 = male; 0 = female')

"""# B ) Age"""

sns.countplot(x='age',data=heart);

"""# machine learning algo

# import library and split data
"""

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import accuracy_score

x=heart.drop('target',axis=1)
y=heart.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)

print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)

"""# 1)	Linear Regression"""

from sklearn import linear_model

#Train the model
model = linear_model.LinearRegression()

#Fit the model
model.fit(x_train, y_train)

#Score/Accuracy
print("Accuracy --> ", model.score(x_test, y_test)*100)

"""# 2) logistic regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

#Fit the model
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

#Score/Accuracy
print("Accuracy --> ", model.score(x_test, y_test)*100)

"""# 3 ) Random Forest Regression"""

from sklearn.ensemble import RandomForestRegressor

#Train the model
model = RandomForestRegressor(n_estimators=1000)

#Fit
model.fit(x_train, y_train)

#Score/Accuracy
print("Accuracy --> ", model.score(x_test, y_test)*100)

"""# 4) Gradient Boosting Regresso"""

from sklearn.ensemble import GradientBoostingRegressor

#Train the model
GBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)

#Fit
GBR.fit(x_train, y_train)

print("Accuracy --> ", GBR.score(x_test, y_test)*100)

"""# 5) KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=20)

knn.fit(x_train, y_train)

y_pred = knn.predict(x_test)

#Score/Accuracy
print("Accuracy --> ", knn.score(x_test, y_test)*100)

"""# 6) decision tree"""

from sklearn import tree

t = tree.DecisionTreeClassifier()

t.fit(x_train,y_train)

y_pred = t.predict(x_test)

#Score/Accuracy
print("Accuracy --> ", t.score(x_test, y_test)*100)

"""# 7) XGBoost"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error

xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 10)

xg_reg.fit(x_train,y_train)

preds = xg_reg.predict(x_test)

#Score/Accuracy
print("Accuracy --> ", xg_reg.score(x_test, y_test)*100)

"""### A) mean_squared_error"""

rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

"""### B) Accuracy score"""

#Score/Accuracy
print("Accuracy --> ", xg_reg.score(x_test, y_test)*100)

xgb.plot_importance(xg_reg)
plt.rcParams['figure.figsize'] = [5, 5]
plt.show()

"""# 8) naive-bayes"""

from sklearn.naive_bayes import GaussianNB

model = GaussianNB()

model.fit(x_train,y_train)

#Score/Accuracy
print("Accuracy --> ", model.score(x_test, y_test)*100)

"""# 9) support vactor machine"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(kernel='linear') # Linear Kernel

#Train the model using the training sets
clf.fit(x_train, y_train)

#Predict the response for test dataset
y_pred = clf.predict(x_test)

#Score/Accuracy
print("Accuracy --> ", clf.score(x_test, y_test)*100)

